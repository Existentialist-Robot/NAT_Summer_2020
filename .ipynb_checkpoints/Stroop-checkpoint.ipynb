{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\pyglet\\media\\codecs\\wmf.py:838: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "from mne import Epochs, find_events\n",
    "from time import time, strftime, gmtime\n",
    "import os\n",
    "import stroopy\n",
    "import record\n",
    "import matplotlib.patches as patches\n",
    "from utils import utils\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyOpenBCI import OpenBCICyton\n",
    "from pylsl import StreamInfo, StreamOutlet, StreamInlet, resolve_byprop\n",
    "from vispy import gloo, app, visuals\n",
    "import math\n",
    "from seaborn import color_palette\n",
    "from scipy.signal import lfilter, lfilter_zi\n",
    "from mne.filter import create_filter\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "from utils import utils\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyOpenBCI import OpenBCICyton\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connect to an EEG Device (OpenBCI)\n",
    "If there IS OpenBCI to connect to --> run the following block, otherwise skip the next block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import stream\n",
    "address = '123456789'\n",
    "name = 'EdenBCI'\n",
    "stream_process = Process(target=stream.stream, args=(address,name))\n",
    "stream_process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or Simulate Data\n",
    "If there is NO OpenBCI to connect to --> open another anaconda instance, activate env and run the send.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for an EEG stream...\n",
      "Start acquiring data.\n",
      "Setting up band-pass filter from 3 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 3.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 2.00 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 165 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835a24ddbf4c4b90b605cd9e7e11ba59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VispyWidget(height=600, width=800)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import view\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply the EEG Device and Wait for Signal Quality to Stabilize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Define these parameters \n",
    "duration = 4 # in seconds. 120 is recommended\n",
    "subject = 2 # unique id for each participant\n",
    "session = 3 # represents a data collection session. Multiple trials can be performed for each session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seat the subject in front of the computer and run the following cell to run a single trial of the experiment.\n",
    "\n",
    "In order to maximise the possibility of success, participants should take the experiment in a quiet environment and do their best to minimize movement that might contaminate the signal. With their jaw and face relaxed, subjects should focus on the stimuli, mentally noting whether they see a \"face\" or a \"house\".\n",
    "\n",
    "Data will be recorded into CSV files in the `eeg-notebooks/data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording data to:  C:\\Users\\Tanya\\eeg-notebooks\\data\\visual\\Stroop\\subject_2\\session_3\\recording_2020-07-02-22.37.56.csv\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\asyncio\\base_events.py\", line 539, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1775, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\vispy\\app\\backends\\_ipynb_webgl.py\", line 200, in _draw_event\n",
      "    self._vispy_canvas.events.draw()\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\vispy\\util\\event.py\", line 455, in __call__\n",
      "    self._invoke_callback(cb, event)\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\vispy\\util\\event.py\", line 475, in _invoke_callback\n",
      "    self, cb_event=(cb, event))\n",
      "  << caught exception here: >>\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\vispy\\util\\event.py\", line 471, in _invoke_callback\n",
      "    cb(event)\n",
      "  File \"C:\\Users\\Tanya\\Desktop\\NAT_Summer_2020\\view.py\", line 253, in on_draw\n",
      "    [t.draw() for t in self.names + self.quality]\n",
      "  File \"C:\\Users\\Tanya\\Desktop\\NAT_Summer_2020\\view.py\", line 253, in <listcomp>\n",
      "    [t.draw() for t in self.names + self.quality]\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\vispy\\visuals\\visual.py\", line 435, in draw\n",
      "    if self._prepare_draw(view=self) is False:\n",
      "  File \"C:\\Users\\Tanya\\Anaconda3\\lib\\site-packages\\vispy\\visuals\\text\\text.py\", line 588, in _prepare_draw\n",
      "    n_pix = (self._font_size / 72.) * transforms.dpi  # logical pix\n",
      "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n",
      "ERROR: Invoking <bound method Canvas.on_draw of <Canvas (ipynb_webgl) at 0x21839ed6940>> for DrawEvent\n",
      "ERROR: Invoking <bound method Canvas.on_draw of <Canvas (ipynb_webgl) at 0x21839ed6940>> repeat 2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path # path creating module (python>=3.5)\n",
    "\n",
    "#our path\n",
    "recording_path = os.path.join(os.path.expanduser(\"~\"), \"eeg-notebooks\", \"data\", \"visual\", \"Stroop\", \"subject\" + \"_\" + str(subject), \"session\" + \"_\" + str(session))\n",
    "\n",
    "#our filename\n",
    "filename = \"recording_%s.csv\" % strftime(\"%Y-%m-%d-%H.%M.%S\", gmtime())\n",
    "\n",
    "#full path\n",
    "fullname = os.path.join(recording_path, filename)\n",
    "\n",
    "#creation of the specified path if it's nonexistent\n",
    "Path(recording_path).mkdir(parents=True, exist_ok=True)\n",
    "print('Recording data to: ', fullname)\n",
    "\n",
    "#creation of the file\n",
    "file = open(fullname, 'w')\n",
    "\n",
    "\n",
    "stimulus = Process(target=stroopy.present, args=(duration,))\n",
    "recording = Process(target=record.record, args=(duration, fullname))\n",
    "stimulus.start()\n",
    "recording.start()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat Data Collection 3-6 times\n",
    "\n",
    "Visualizing ERPs requires averaging the EEG response over many different rounds of stimulus presentation. Depending on experimental conditions, this may require as little as one two minute trial or as many as 6. We recommend repeating the above experiment 3-6 times before proceeding. \n",
    "\n",
    "Make sure to take breaks, though! Inattention, fatigue, and distraction will decrease the quality of event-related potentials such as the N170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## Step 4: Prepare the Data for Analysis\n",
    "\n",
    "Once a suitable data set has been collected, it is now time to analyze the data and see if we can identify the N170\n",
    "\n",
    "\n",
    "### Load data into MNE objects\n",
    "\n",
    "[MNE](https://martinos.org/mne/stable/index.html) is a very powerful Python library for analyzing EEG data. It provides helpful functions for performing key tasks such as filtering EEG data, rejecting artifacts, and grouping EEG data into chunks (epochs).\n",
    "\n",
    "The first step to using MNE is to read the data we've collected into an MNE `Raw` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid value for the 'fname' parameter. Allowed values are '.loc', '.locs', '.eloc', '.sfp', '.csd', '.elc', '.txt', '.elp' and '.bvef', but got '' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0eadea3776cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubject_nb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession_nb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\NAT_Summer_2020\\utils\\utils.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(subject_nb, session_nb, sfreq, ch_ind, stim_ind, replace_ch_names, verbose)\u001b[0m\n\u001b[0;32m     98\u001b[0m     return load_openBCI_csv_as_raw(fnames, sfreq=sfreq, ch_ind=ch_ind,\n\u001b[0;32m     99\u001b[0m                                 \u001b[0mstim_ind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstim_ind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                                 replace_ch_names=replace_ch_names, verbose=verbose)\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m def plot_conditions(epochs, conditions=OrderedDict(), ci=97.5, n_boot=1000,\n",
      "\u001b[1;32m~\\Desktop\\NAT_Summer_2020\\utils\\utils.py\u001b[0m in \u001b[0;36mload_openBCI_csv_as_raw\u001b[1;34m(filename, sfreq, ch_ind, stim_ind, replace_ch_names, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# type of each channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mch_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'eeg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_channel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'stim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mmontage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_custom_montage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'standard_1005'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;31m# get data and exclude Aux channel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch_ind\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstim_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mne\\channels\\montage.py\u001b[0m in \u001b[0;36mread_custom_montage\u001b[1;34m(fname, head_size, coord_frame)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m     \u001b[0m_check_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fname'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mSUPPORTED_FILE_EXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSUPPORTED_FILE_EXT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eeglab'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mne\\utils\\check.py\u001b[0m in \u001b[0;36m_check_option\u001b[1;34m(parameter, value, allowed_values, extra)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m' and %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mallowed_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     raise ValueError(msg.format(parameter=parameter, options=options,\n\u001b[1;32m--> 565\u001b[1;33m                                 value=value, extra=extra))\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid value for the 'fname' parameter. Allowed values are '.loc', '.locs', '.eloc', '.sfp', '.csd', '.elc', '.txt', '.elp' and '.bvef', but got '' instead."
     ]
    }
   ],
   "source": [
    "raw = utils.load_data(sfreq=256.,subject_nb=subject, session_nb=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Power Spectrum\n",
    "\n",
    "Plotting the power spectral density (PSD) of our dataset will give us a glimpse at the different frequencies that are present. We won't be able to see the N170 in the PSD, but it will give us an impression of how noisy our data was. A very noisy or flat PSD may represent poor signal quality at certain electrodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "raw.plot_psd();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PSD looks good. There is a large peak at 60hz, representing background electrical activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "Most ERP components are composed of lower frequency fluctuations in the EEG signal. Thus, we can filter out all frequencies between 1 and 30 hz in order to increase our ability to detect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.filter(1,30, method='iir')\n",
    "raw.plot_psd(fmin=1, fmax=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PSD of frequencies between 1 and 30 hz looks good. The difference between the temporal channels (red and black) and the frontal channels (blue and green) is clearly evident. The huge peak from 1 to 3hz is largely due to the presence of eye blinks, which produce large amplitude, low-frequency events in the EEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoching\n",
    "\n",
    "Next, we will chunk (epoch) the data into segments representing the data 100ms before to 800ms after each stimulus. No baseline correction is needed (signal is bandpass filtered) and we will reject every epoch where the amplitude of the signal exceeded 75 uV, which should most eye blinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array containing the timestamps and type of each stimulus (i.e. face or house)\n",
    "events = find_events(raw)\n",
    "event_id = {'House': 1, 'Face': 2}\n",
    "\n",
    "# Create an MNE Epochs object representing all the epochs around stimulus presentation\n",
    "epochs = Epochs(raw, events=events, event_id=event_id, \n",
    "                tmin=-0.1, tmax=0.8, baseline=None,\n",
    "                reject={'eeg': 75e-6}, preload=True, \n",
    "                verbose=False, picks=[0,1,2,3])\n",
    "print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample drop % is an important metric representing how noisy our data set was. If this is greater than 20%, consider ensuring that signal variances is very low in the raw EEG viewer and collecting more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze the Data\n",
    "\n",
    "Finally, we can now analyze our results by averaging the epochs that occured during the different stimuli and looking for differences in the waveform\n",
    "\n",
    "\n",
    "### Epoch average\n",
    "\n",
    "With our `plot_conditions` utility function, we can plot the average ERP for all electrodes for both conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "conditions = OrderedDict()\n",
    "conditions['House'] = [1]\n",
    "conditions['Face'] = [2]\n",
    "\n",
    "fig, ax = utils.plot_conditions(epochs, conditions=conditions, \n",
    "                                ci=97.5, n_boot=1000, title='',\n",
    "                                diff_waveform=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a very nice deflection in the temporal channels around 200ms for face stimuli. This is likely the N170, although appearing slightly later due to delay in receiving the data over bluetooth. \n",
    "\n",
    "There's not much to see in the frontal channels (AF7 and AF8), but that's to be expected based on the fact that the N170 is mostly a lateral posterior brain phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the N170\n",
    "\n",
    "Next, we will use 4 different machine learning pipelines to classify the N170 based on the data we collected. The \n",
    "\n",
    "- **Vect + LR** :  Vectorization of the trial + Logistic Regression. This can be considered the standard decoding pipeline for MEG / EEG.\n",
    "- **Vect + RegLDA** :  Vectorization of the trial + Regularized LDA. This one is very commonly used in P300 BCIs. It can outperform the previous one but become unusable if the number of dimension is too high.\n",
    "- **ERPCov + TS**: ErpCovariance + Tangent space mapping. One of the most reliable Riemannian geometry-based pipeline.\n",
    "- **ERPCov + MDM**: ErpCovariance + MDM. A very simple, yet effective (for low channel count), Riemannian geometry classifier.\n",
    "\n",
    "Evaluation is done through cross-validation, with area-under-the-curve (AUC) as metric (AUC is probably the best metric for binary and unbalanced classification problem)\n",
    "\n",
    "*Note: because we're doing machine learning here, the following cell may take a while to complete*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from pyriemann.estimation import ERPCovariances, XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "clfs = OrderedDict()\n",
    "\n",
    "clfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\n",
    "clfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n",
    "clfs['ERPCov + TS'] = make_pipeline(ERPCovariances(estimator='oas'), TangentSpace(), LogisticRegression())\n",
    "clfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(estimator='oas'), MDM())\n",
    "clfs['XdawnCov + TS'] = make_pipeline(XdawnCovariances(estimator='oas'), TangentSpace(), LogisticRegression())\n",
    "clfs['XdawnCov + MDM'] = make_pipeline(XdawnCovariances(estimator='oas'), MDM())\n",
    "\n",
    "# format data\n",
    "epochs.pick_types(eeg=True)\n",
    "X = epochs.get_data() * 1e6\n",
    "times = epochs.times\n",
    "y = epochs.events[:, -1]\n",
    "\n",
    "# define cross validation \n",
    "cv = StratifiedShuffleSplit(n_splits=20, test_size=0.25, \n",
    "                            random_state=42)\n",
    "\n",
    "# run cross validation for each pipeline\n",
    "auc = []\n",
    "methods = []\n",
    "for m in clfs:\n",
    "    print(m)\n",
    "    try:\n",
    "        \n",
    "        res = cross_val_score(clfs[m], X, y==2, scoring='roc_auc', \n",
    "                              cv=cv, n_jobs=-1)\n",
    "        auc.extend(res)\n",
    "        methods.extend([m]*len(res))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Decoding Results\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "    \n",
    "results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "results['Method'] = methods\n",
    "\n",
    "fig = plt.figure(figsize=[8,4])\n",
    "sns.barplot(data=results, x='AUC', y='Method')\n",
    "plt.xlim(0.4, 0.9)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classifiers for this data set appear to be the ERPCov and XdawnCov with tangent space projection pipelines. AUC is around .7, which is good, but on the low end for being able to run a brain-computer interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Share your Data!\n",
    "\n",
    "How did your experiment go? If you're excited by your results we'd love to see your data!\n",
    "\n",
    "Follow the instructions on our [Contributions](https://github.com/NeuroTechX/eeg-notebooks/blob/master/CONTRIBUTING.md) page to make a pull request with your data and we'll review it to be added to the EEG notebooks project."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mne"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
